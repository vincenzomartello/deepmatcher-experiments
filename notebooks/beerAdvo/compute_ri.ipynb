{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utilities_functions.intermediate_layer_extraction import return_layer_input_output\n",
    "from utilities_functions.ri_calculator import find_smallest_variation_to_change\n",
    "import deepmatcher as dm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes =['Beer_Name','Brew_Factory_Name','Style','ABV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MatchingModel(\n",
       "  (attr_summarizers): ModuleMap(\n",
       "    (Beer_Name): Hybrid(\n",
       "      (word_contextualizer): RNN(\n",
       "        (rnn_groups): ModuleList(\n",
       "          (0): GRU(300, 150, batch_first=True, bidirectional=True)\n",
       "        )\n",
       "        (dropouts): ModuleList(\n",
       "          (0): Dropout(p=0)\n",
       "        )\n",
       "        (bypass_networks): ModuleList(\n",
       "          (0): None\n",
       "        )\n",
       "        (input_dropout): NoMeta(\n",
       "          (module): Dropout(p=0)\n",
       "        )\n",
       "      )\n",
       "      (word_comparator): Attention(\n",
       "        (alignment_networks): ModuleList(\n",
       "          (0): AlignmentNetwork(\n",
       "            (transform): Transform(\n",
       "              (transforms): ModuleList(\n",
       "                (0): Linear(in_features=300, out_features=300, bias=True)\n",
       "                (1): Linear(in_features=300, out_features=300, bias=True)\n",
       "              )\n",
       "              (bypass_networks): ModuleList(\n",
       "                (0): Bypass(\n",
       "                  (highway_gate): Linear(in_features=300, out_features=300, bias=True)\n",
       "                )\n",
       "                (1): Bypass(\n",
       "                  (highway_gate): Linear(in_features=300, out_features=300, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (value_merge): Merge(\n",
       "        )\n",
       "        (comparison_merge): Merge(\n",
       "        )\n",
       "        (comparison_network): Transform(\n",
       "          (transforms): ModuleList(\n",
       "            (0): Linear(in_features=600, out_features=300, bias=True)\n",
       "            (1): Linear(in_features=300, out_features=300, bias=True)\n",
       "          )\n",
       "          (bypass_networks): ModuleList(\n",
       "            (0): Bypass(\n",
       "              (highway_gate): Linear(in_features=600, out_features=300, bias=True)\n",
       "            )\n",
       "            (1): Bypass(\n",
       "              (highway_gate): Linear(in_features=300, out_features=300, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_dropout): Dropout(p=0)\n",
       "        (transform_dropout): Dropout(p=0)\n",
       "        (score_dropout): Dropout(p=0)\n",
       "        (softmax): Softmax()\n",
       "      )\n",
       "      (word_aggregator): AttentionWithRNN(\n",
       "        (rnn): RNN(\n",
       "          (rnn_groups): ModuleList(\n",
       "            (0): GRU(300, 150, batch_first=True, bidirectional=True)\n",
       "          )\n",
       "          (dropouts): ModuleList(\n",
       "            (0): Dropout(p=0)\n",
       "          )\n",
       "          (bypass_networks): ModuleList(\n",
       "            (0): None\n",
       "          )\n",
       "          (input_dropout): NoMeta(\n",
       "            (module): Dropout(p=0)\n",
       "          )\n",
       "        )\n",
       "        (rnn_pool): Pool(\n",
       "        )\n",
       "        (input_context_comparison_network): Transform(\n",
       "          (transforms): ModuleList(\n",
       "            (0): Linear(in_features=600, out_features=300, bias=True)\n",
       "          )\n",
       "          (bypass_networks): ModuleList(\n",
       "            (0): Bypass(\n",
       "              (highway_gate): Linear(in_features=600, out_features=300, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (scoring_network): Transform(\n",
       "          (transforms): ModuleList(\n",
       "            (0): Linear(in_features=300, out_features=1, bias=True)\n",
       "          )\n",
       "          (bypass_networks): ModuleList(\n",
       "            (0): None\n",
       "          )\n",
       "        )\n",
       "        (input_dropout): Dropout(p=0)\n",
       "        (transform_dropout): Dropout(p=0)\n",
       "        (score_dropout): Dropout(p=0)\n",
       "        (softmax): Softmax()\n",
       "      )\n",
       "    )\n",
       "    (Brew_Factory_Name): Hybrid(\n",
       "      (word_contextualizer): RNN(\n",
       "        (rnn_groups): ModuleList(\n",
       "          (0): GRU(300, 150, batch_first=True, bidirectional=True)\n",
       "        )\n",
       "        (dropouts): ModuleList(\n",
       "          (0): Dropout(p=0)\n",
       "        )\n",
       "        (bypass_networks): ModuleList(\n",
       "          (0): None\n",
       "        )\n",
       "        (input_dropout): NoMeta(\n",
       "          (module): Dropout(p=0)\n",
       "        )\n",
       "      )\n",
       "      (word_comparator): Attention(\n",
       "        (alignment_networks): ModuleList(\n",
       "          (0): AlignmentNetwork(\n",
       "            (transform): Transform(\n",
       "              (transforms): ModuleList(\n",
       "                (0): Linear(in_features=300, out_features=300, bias=True)\n",
       "                (1): Linear(in_features=300, out_features=300, bias=True)\n",
       "              )\n",
       "              (bypass_networks): ModuleList(\n",
       "                (0): Bypass(\n",
       "                  (highway_gate): Linear(in_features=300, out_features=300, bias=True)\n",
       "                )\n",
       "                (1): Bypass(\n",
       "                  (highway_gate): Linear(in_features=300, out_features=300, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (value_merge): Merge(\n",
       "        )\n",
       "        (comparison_merge): Merge(\n",
       "        )\n",
       "        (comparison_network): Transform(\n",
       "          (transforms): ModuleList(\n",
       "            (0): Linear(in_features=600, out_features=300, bias=True)\n",
       "            (1): Linear(in_features=300, out_features=300, bias=True)\n",
       "          )\n",
       "          (bypass_networks): ModuleList(\n",
       "            (0): Bypass(\n",
       "              (highway_gate): Linear(in_features=600, out_features=300, bias=True)\n",
       "            )\n",
       "            (1): Bypass(\n",
       "              (highway_gate): Linear(in_features=300, out_features=300, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_dropout): Dropout(p=0)\n",
       "        (transform_dropout): Dropout(p=0)\n",
       "        (score_dropout): Dropout(p=0)\n",
       "        (softmax): Softmax()\n",
       "      )\n",
       "      (word_aggregator): AttentionWithRNN(\n",
       "        (rnn): RNN(\n",
       "          (rnn_groups): ModuleList(\n",
       "            (0): GRU(300, 150, batch_first=True, bidirectional=True)\n",
       "          )\n",
       "          (dropouts): ModuleList(\n",
       "            (0): Dropout(p=0)\n",
       "          )\n",
       "          (bypass_networks): ModuleList(\n",
       "            (0): None\n",
       "          )\n",
       "          (input_dropout): NoMeta(\n",
       "            (module): Dropout(p=0)\n",
       "          )\n",
       "        )\n",
       "        (rnn_pool): Pool(\n",
       "        )\n",
       "        (input_context_comparison_network): Transform(\n",
       "          (transforms): ModuleList(\n",
       "            (0): Linear(in_features=600, out_features=300, bias=True)\n",
       "          )\n",
       "          (bypass_networks): ModuleList(\n",
       "            (0): Bypass(\n",
       "              (highway_gate): Linear(in_features=600, out_features=300, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (scoring_network): Transform(\n",
       "          (transforms): ModuleList(\n",
       "            (0): Linear(in_features=300, out_features=1, bias=True)\n",
       "          )\n",
       "          (bypass_networks): ModuleList(\n",
       "            (0): None\n",
       "          )\n",
       "        )\n",
       "        (input_dropout): Dropout(p=0)\n",
       "        (transform_dropout): Dropout(p=0)\n",
       "        (score_dropout): Dropout(p=0)\n",
       "        (softmax): Softmax()\n",
       "      )\n",
       "    )\n",
       "    (Style): Hybrid(\n",
       "      (word_contextualizer): RNN(\n",
       "        (rnn_groups): ModuleList(\n",
       "          (0): GRU(300, 150, batch_first=True, bidirectional=True)\n",
       "        )\n",
       "        (dropouts): ModuleList(\n",
       "          (0): Dropout(p=0)\n",
       "        )\n",
       "        (bypass_networks): ModuleList(\n",
       "          (0): None\n",
       "        )\n",
       "        (input_dropout): NoMeta(\n",
       "          (module): Dropout(p=0)\n",
       "        )\n",
       "      )\n",
       "      (word_comparator): Attention(\n",
       "        (alignment_networks): ModuleList(\n",
       "          (0): AlignmentNetwork(\n",
       "            (transform): Transform(\n",
       "              (transforms): ModuleList(\n",
       "                (0): Linear(in_features=300, out_features=300, bias=True)\n",
       "                (1): Linear(in_features=300, out_features=300, bias=True)\n",
       "              )\n",
       "              (bypass_networks): ModuleList(\n",
       "                (0): Bypass(\n",
       "                  (highway_gate): Linear(in_features=300, out_features=300, bias=True)\n",
       "                )\n",
       "                (1): Bypass(\n",
       "                  (highway_gate): Linear(in_features=300, out_features=300, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (value_merge): Merge(\n",
       "        )\n",
       "        (comparison_merge): Merge(\n",
       "        )\n",
       "        (comparison_network): Transform(\n",
       "          (transforms): ModuleList(\n",
       "            (0): Linear(in_features=600, out_features=300, bias=True)\n",
       "            (1): Linear(in_features=300, out_features=300, bias=True)\n",
       "          )\n",
       "          (bypass_networks): ModuleList(\n",
       "            (0): Bypass(\n",
       "              (highway_gate): Linear(in_features=600, out_features=300, bias=True)\n",
       "            )\n",
       "            (1): Bypass(\n",
       "              (highway_gate): Linear(in_features=300, out_features=300, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_dropout): Dropout(p=0)\n",
       "        (transform_dropout): Dropout(p=0)\n",
       "        (score_dropout): Dropout(p=0)\n",
       "        (softmax): Softmax()\n",
       "      )\n",
       "      (word_aggregator): AttentionWithRNN(\n",
       "        (rnn): RNN(\n",
       "          (rnn_groups): ModuleList(\n",
       "            (0): GRU(300, 150, batch_first=True, bidirectional=True)\n",
       "          )\n",
       "          (dropouts): ModuleList(\n",
       "            (0): Dropout(p=0)\n",
       "          )\n",
       "          (bypass_networks): ModuleList(\n",
       "            (0): None\n",
       "          )\n",
       "          (input_dropout): NoMeta(\n",
       "            (module): Dropout(p=0)\n",
       "          )\n",
       "        )\n",
       "        (rnn_pool): Pool(\n",
       "        )\n",
       "        (input_context_comparison_network): Transform(\n",
       "          (transforms): ModuleList(\n",
       "            (0): Linear(in_features=600, out_features=300, bias=True)\n",
       "          )\n",
       "          (bypass_networks): ModuleList(\n",
       "            (0): Bypass(\n",
       "              (highway_gate): Linear(in_features=600, out_features=300, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (scoring_network): Transform(\n",
       "          (transforms): ModuleList(\n",
       "            (0): Linear(in_features=300, out_features=1, bias=True)\n",
       "          )\n",
       "          (bypass_networks): ModuleList(\n",
       "            (0): None\n",
       "          )\n",
       "        )\n",
       "        (input_dropout): Dropout(p=0)\n",
       "        (transform_dropout): Dropout(p=0)\n",
       "        (score_dropout): Dropout(p=0)\n",
       "        (softmax): Softmax()\n",
       "      )\n",
       "    )\n",
       "    (ABV): Hybrid(\n",
       "      (word_contextualizer): RNN(\n",
       "        (rnn_groups): ModuleList(\n",
       "          (0): GRU(300, 150, batch_first=True, bidirectional=True)\n",
       "        )\n",
       "        (dropouts): ModuleList(\n",
       "          (0): Dropout(p=0)\n",
       "        )\n",
       "        (bypass_networks): ModuleList(\n",
       "          (0): None\n",
       "        )\n",
       "        (input_dropout): NoMeta(\n",
       "          (module): Dropout(p=0)\n",
       "        )\n",
       "      )\n",
       "      (word_comparator): Attention(\n",
       "        (alignment_networks): ModuleList(\n",
       "          (0): AlignmentNetwork(\n",
       "            (transform): Transform(\n",
       "              (transforms): ModuleList(\n",
       "                (0): Linear(in_features=300, out_features=300, bias=True)\n",
       "                (1): Linear(in_features=300, out_features=300, bias=True)\n",
       "              )\n",
       "              (bypass_networks): ModuleList(\n",
       "                (0): Bypass(\n",
       "                  (highway_gate): Linear(in_features=300, out_features=300, bias=True)\n",
       "                )\n",
       "                (1): Bypass(\n",
       "                  (highway_gate): Linear(in_features=300, out_features=300, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (value_merge): Merge(\n",
       "        )\n",
       "        (comparison_merge): Merge(\n",
       "        )\n",
       "        (comparison_network): Transform(\n",
       "          (transforms): ModuleList(\n",
       "            (0): Linear(in_features=600, out_features=300, bias=True)\n",
       "            (1): Linear(in_features=300, out_features=300, bias=True)\n",
       "          )\n",
       "          (bypass_networks): ModuleList(\n",
       "            (0): Bypass(\n",
       "              (highway_gate): Linear(in_features=600, out_features=300, bias=True)\n",
       "            )\n",
       "            (1): Bypass(\n",
       "              (highway_gate): Linear(in_features=300, out_features=300, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_dropout): Dropout(p=0)\n",
       "        (transform_dropout): Dropout(p=0)\n",
       "        (score_dropout): Dropout(p=0)\n",
       "        (softmax): Softmax()\n",
       "      )\n",
       "      (word_aggregator): AttentionWithRNN(\n",
       "        (rnn): RNN(\n",
       "          (rnn_groups): ModuleList(\n",
       "            (0): GRU(300, 150, batch_first=True, bidirectional=True)\n",
       "          )\n",
       "          (dropouts): ModuleList(\n",
       "            (0): Dropout(p=0)\n",
       "          )\n",
       "          (bypass_networks): ModuleList(\n",
       "            (0): None\n",
       "          )\n",
       "          (input_dropout): NoMeta(\n",
       "            (module): Dropout(p=0)\n",
       "          )\n",
       "        )\n",
       "        (rnn_pool): Pool(\n",
       "        )\n",
       "        (input_context_comparison_network): Transform(\n",
       "          (transforms): ModuleList(\n",
       "            (0): Linear(in_features=600, out_features=300, bias=True)\n",
       "          )\n",
       "          (bypass_networks): ModuleList(\n",
       "            (0): Bypass(\n",
       "              (highway_gate): Linear(in_features=600, out_features=300, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (scoring_network): Transform(\n",
       "          (transforms): ModuleList(\n",
       "            (0): Linear(in_features=300, out_features=1, bias=True)\n",
       "          )\n",
       "          (bypass_networks): ModuleList(\n",
       "            (0): None\n",
       "          )\n",
       "        )\n",
       "        (input_dropout): Dropout(p=0)\n",
       "        (transform_dropout): Dropout(p=0)\n",
       "        (score_dropout): Dropout(p=0)\n",
       "        (softmax): Softmax()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (attr_summarizer): Hybrid(\n",
       "  )\n",
       "  (attr_condensors): ModuleMap(\n",
       "    (Beer_Name): Transform(\n",
       "      (transforms): ModuleList(\n",
       "        (0): Linear(in_features=300, out_features=75, bias=True)\n",
       "      )\n",
       "      (bypass_networks): ModuleList(\n",
       "        (0): Bypass(\n",
       "          (highway_gate): Linear(in_features=300, out_features=75, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (Brew_Factory_Name): Transform(\n",
       "      (transforms): ModuleList(\n",
       "        (0): Linear(in_features=300, out_features=75, bias=True)\n",
       "      )\n",
       "      (bypass_networks): ModuleList(\n",
       "        (0): Bypass(\n",
       "          (highway_gate): Linear(in_features=300, out_features=75, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (Style): Transform(\n",
       "      (transforms): ModuleList(\n",
       "        (0): Linear(in_features=300, out_features=75, bias=True)\n",
       "      )\n",
       "      (bypass_networks): ModuleList(\n",
       "        (0): Bypass(\n",
       "          (highway_gate): Linear(in_features=300, out_features=75, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ABV): Transform(\n",
       "      (transforms): ModuleList(\n",
       "        (0): Linear(in_features=300, out_features=75, bias=True)\n",
       "      )\n",
       "      (bypass_networks): ModuleList(\n",
       "        (0): Bypass(\n",
       "          (highway_gate): Linear(in_features=300, out_features=75, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (attr_comparators): ModuleMap(\n",
       "    (Beer_Name): Merge(\n",
       "    )\n",
       "    (Brew_Factory_Name): Merge(\n",
       "    )\n",
       "    (Style): Merge(\n",
       "    )\n",
       "    (ABV): Merge(\n",
       "    )\n",
       "  )\n",
       "  (attr_comparator): Merge(\n",
       "  )\n",
       "  (attr_merge): Merge(\n",
       "  )\n",
       "  (classifier): Classifier(\n",
       "    (transform): Transform(\n",
       "      (transforms): ModuleList(\n",
       "        (0): Linear(in_features=900, out_features=300, bias=True)\n",
       "        (1): Linear(in_features=300, out_features=300, bias=True)\n",
       "      )\n",
       "      (bypass_networks): ModuleList(\n",
       "        (0): Bypass(\n",
       "          (highway_gate): Linear(in_features=900, out_features=300, bias=True)\n",
       "        )\n",
       "        (1): Bypass(\n",
       "          (highway_gate): Linear(in_features=300, out_features=300, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (softmax_transform): Transform(\n",
       "      (transforms): ModuleList(\n",
       "        (0): Linear(in_features=300, out_features=2, bias=True)\n",
       "      )\n",
       "      (bypass_networks): ModuleList(\n",
       "        (0): None\n",
       "      )\n",
       "    )\n",
       "    (softmax): LogSoftmax()\n",
       "  )\n",
       "  (embed): ModuleMap(\n",
       "    (ltable_Beer_Name): NoMeta(\n",
       "      (module): Embedding(1302, 300)\n",
       "    )\n",
       "    (ltable_Brew_Factory_Name): NoMeta(\n",
       "      (module): Embedding(1302, 300)\n",
       "    )\n",
       "    (ltable_Style): NoMeta(\n",
       "      (module): Embedding(1302, 300)\n",
       "    )\n",
       "    (ltable_ABV): NoMeta(\n",
       "      (module): Embedding(1302, 300)\n",
       "    )\n",
       "    (rtable_Beer_Name): NoMeta(\n",
       "      (module): Embedding(1302, 300)\n",
       "    )\n",
       "    (rtable_Brew_Factory_Name): NoMeta(\n",
       "      (module): Embedding(1302, 300)\n",
       "    )\n",
       "    (rtable_Style): NoMeta(\n",
       "      (module): Embedding(1302, 300)\n",
       "    )\n",
       "    (rtable_ABV): NoMeta(\n",
       "      (module): Embedding(1302, 300)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = dm.MatchingModel(attr_summarizer='hybrid')\n",
    "model.load_state('../../models/beer_hybrid.pth')\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_classifier_inputs,neg_classifier_outputs,neg_ids = return_layer_input_output('../../Structured/Beer/'\n",
    "                                                                         ,'negatives',32,model,\n",
    "                                                                        model.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "testneg_classifier_inputs,testneg_classifier_outputs,testneg_ids = return_layer_input_output('../../Structured/Beer/'\n",
    "                                                                         ,'test_negatives',32,model,\n",
    "                                                                        model.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_classifier_inputs,pos_classifier_outputs,pos_ids = return_layer_input_output('../../Structured/Beer',\n",
    "                                                                       'positives',32,model,\n",
    "                                                                       model.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "testpos_classifier_inputs,testpos_classifier_outputs,testpos_ids = return_layer_input_output('../../Structured/Beer',\n",
    "                                                                       'test_positives',32,model,\n",
    "                                                                       model.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_classifier_inputs = list(map(lambda x: x[0],neg_classifier_inputs))\n",
    "positive_classifier_inputs = list(map(lambda x: x[0],pos_classifier_inputs))\n",
    "test_negative_classifier_inputs = list(map(lambda x: x[0],testneg_classifier_inputs))\n",
    "test_positive_classifier_inputs = list(map(lambda x: x[0],testpos_classifier_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_length = int((300*3)/len(attributes))\n",
    "classifier_length = int(attribute_length*len(attributes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sample number 0\n",
      "Processing sample number 1\n",
      "Processing sample number 2\n",
      "Processing sample number 3\n",
      "Processing sample number 4\n",
      "Processing sample number 5\n",
      "Processing sample number 6\n",
      "Processing sample number 7\n",
      "Processing sample number 8\n",
      "Processing sample number 9\n",
      "Processing sample number 10\n",
      "Processing sample number 11\n",
      "Processing sample number 12\n",
      "Processing sample number 13\n",
      "Processing sample number 14\n",
      "Processing sample number 15\n",
      "Processing sample number 16\n",
      "Processing sample number 17\n",
      "Processing sample number 18\n",
      "Processing sample number 19\n",
      "Processing sample number 20\n",
      "Processing sample number 21\n",
      "Processing sample number 22\n",
      "Processing sample number 23\n",
      "Processing sample number 24\n",
      "Processing sample number 25\n",
      "Processing sample number 26\n",
      "Processing sample number 27\n",
      "Processing sample number 28\n",
      "Processing sample number 29\n",
      "Processing sample number 30\n",
      "Processing sample number 31\n",
      "Processing sample number 32\n",
      "Processing sample number 33\n",
      "Processing sample number 34\n",
      "Processing sample number 35\n",
      "Processing sample number 36\n",
      "Processing sample number 37\n",
      "Processing sample number 38\n",
      "Processing sample number 39\n",
      "Processing sample number 40\n",
      "Processing sample number 41\n",
      "Processing sample number 42\n",
      "Processing sample number 43\n",
      "Processing sample number 44\n",
      "Processing sample number 45\n",
      "Processing sample number 46\n",
      "Processing sample number 47\n",
      "Processing sample number 48\n",
      "Processing sample number 49\n",
      "Processing sample number 50\n",
      "Processing sample number 51\n",
      "Processing sample number 52\n",
      "Processing sample number 53\n",
      "Processing sample number 54\n",
      "Processing sample number 55\n",
      "Processing sample number 56\n",
      "Processing sample number 57\n",
      "Processing sample number 58\n",
      "Processing sample number 59\n",
      "Processing sample number 60\n",
      "Processing sample number 61\n",
      "Processing sample number 62\n",
      "Processing sample number 63\n",
      "Processing sample number 64\n",
      "Processing sample number 65\n",
      "Processing sample number 66\n",
      "Processing sample number 67\n",
      "Processing sample number 68\n",
      "Processing sample number 69\n",
      "Processing sample number 70\n",
      "Processing sample number 71\n",
      "Processing sample number 72\n",
      "Processing sample number 73\n",
      "Processing sample number 74\n",
      "Processing sample number 75\n",
      "Processing sample number 76\n",
      "Processing sample number 77\n",
      "Processing sample number 78\n",
      "Processing sample number 79\n",
      "Processing sample number 80\n",
      "Processing sample number 81\n",
      "Processing sample number 82\n",
      "Processing sample number 83\n",
      "Processing sample number 84\n",
      "Processing sample number 85\n",
      "Processing sample number 86\n",
      "Processing sample number 87\n",
      "Processing sample number 88\n",
      "Processing sample number 89\n",
      "Processing sample number 90\n",
      "Processing sample number 91\n",
      "Processing sample number 92\n",
      "Processing sample number 93\n",
      "Processing sample number 94\n",
      "Processing sample number 95\n",
      "Processing sample number 96\n",
      "Processing sample number 97\n",
      "Processing sample number 98\n",
      "Processing sample number 99\n",
      "Processing sample number 100\n",
      "Processing sample number 101\n",
      "Processing sample number 102\n",
      "Processing sample number 103\n",
      "Processing sample number 104\n",
      "Processing sample number 105\n",
      "Processing sample number 106\n",
      "Processing sample number 107\n",
      "Processing sample number 108\n",
      "Processing sample number 109\n",
      "Processing sample number 110\n",
      "Processing sample number 111\n",
      "Processing sample number 112\n",
      "Processing sample number 113\n",
      "Processing sample number 114\n",
      "Processing sample number 115\n",
      "Processing sample number 116\n",
      "Processing sample number 117\n",
      "Processing sample number 118\n",
      "Processing sample number 119\n",
      "Processing sample number 120\n",
      "Processing sample number 121\n",
      "Processing sample number 122\n",
      "Processing sample number 123\n",
      "Processing sample number 124\n",
      "Processing sample number 125\n",
      "Processing sample number 126\n",
      "Processing sample number 127\n",
      "Processing sample number 128\n",
      "Processing sample number 129\n",
      "Processing sample number 130\n",
      "Processing sample number 131\n",
      "Processing sample number 132\n",
      "Processing sample number 133\n",
      "Processing sample number 134\n",
      "Processing sample number 135\n",
      "Processing sample number 136\n",
      "Processing sample number 137\n",
      "Processing sample number 138\n",
      "Processing sample number 139\n",
      "Processing sample number 140\n",
      "Processing sample number 141\n",
      "Processing sample number 142\n",
      "Processing sample number 143\n",
      "Processing sample number 144\n",
      "Processing sample number 145\n",
      "Processing sample number 146\n",
      "Processing sample number 147\n",
      "Processing sample number 148\n",
      "Processing sample number 149\n",
      "Processing sample number 150\n",
      "Processing sample number 151\n",
      "Processing sample number 152\n",
      "Processing sample number 153\n",
      "Processing sample number 154\n",
      "Processing sample number 155\n",
      "Processing sample number 156\n",
      "Processing sample number 157\n",
      "Processing sample number 158\n",
      "Processing sample number 159\n",
      "Processing sample number 160\n",
      "Processing sample number 161\n",
      "Processing sample number 162\n",
      "Processing sample number 163\n",
      "Processing sample number 164\n",
      "Processing sample number 165\n",
      "Processing sample number 166\n",
      "Processing sample number 167\n",
      "Processing sample number 168\n",
      "Processing sample number 169\n",
      "Processing sample number 170\n",
      "Processing sample number 171\n",
      "Processing sample number 172\n",
      "Processing sample number 173\n",
      "Processing sample number 174\n",
      "Processing sample number 175\n",
      "Processing sample number 176\n",
      "Processing sample number 177\n",
      "Processing sample number 178\n",
      "Processing sample number 179\n",
      "Processing sample number 180\n",
      "Processing sample number 181\n",
      "Processing sample number 182\n",
      "Processing sample number 183\n",
      "Processing sample number 184\n",
      "Processing sample number 185\n",
      "Processing sample number 186\n",
      "Processing sample number 187\n",
      "Processing sample number 188\n",
      "Processing sample number 189\n",
      "Processing sample number 190\n",
      "Processing sample number 191\n",
      "Processing sample number 192\n",
      "Processing sample number 193\n",
      "Processing sample number 194\n",
      "Processing sample number 195\n",
      "Processing sample number 196\n",
      "Processing sample number 197\n",
      "Processing sample number 198\n",
      "Processing sample number 199\n",
      "Processing sample number 200\n",
      "Processing sample number 201\n",
      "Processing sample number 202\n",
      "Processing sample number 203\n",
      "Processing sample number 204\n",
      "Processing sample number 205\n",
      "Processing sample number 206\n",
      "Processing sample number 207\n",
      "Processing sample number 208\n",
      "Processing sample number 209\n",
      "Processing sample number 210\n",
      "Processing sample number 211\n",
      "Processing sample number 212\n",
      "Processing sample number 213\n",
      "Processing sample number 214\n",
      "Processing sample number 215\n",
      "Processing sample number 216\n",
      "Processing sample number 217\n",
      "Processing sample number 218\n",
      "Processing sample number 219\n",
      "Processing sample number 220\n",
      "Processing sample number 221\n",
      "Processing sample number 222\n",
      "Processing sample number 223\n",
      "Processing sample number 224\n",
      "Processing sample number 225\n",
      "Processing sample number 226\n",
      "Processing sample number 227\n",
      "Processing sample number 228\n",
      "Processing sample number 229\n",
      "Processing sample number 230\n",
      "Processing sample number 231\n",
      "Processing sample number 232\n",
      "Processing sample number 233\n",
      "Processing sample number 234\n",
      "Processing sample number 235\n",
      "Processing sample number 236\n",
      "Processing sample number 237\n",
      "Processing sample number 238\n",
      "Processing sample number 239\n",
      "Processing sample number 240\n",
      "Processing sample number 241\n",
      "Processing sample number 242\n",
      "Processing sample number 243\n",
      "Processing sample number 244\n",
      "Processing sample number 245\n",
      "Processing sample number 246\n",
      "Processing sample number 247\n",
      "Processing sample number 248\n",
      "Processing sample number 249\n",
      "Processing sample number 250\n",
      "Processing sample number 251\n",
      "Processing sample number 252\n",
      "Processing sample number 253\n",
      "Processing sample number 254\n",
      "Processing sample number 255\n",
      "Processing sample number 256\n",
      "Processing sample number 257\n",
      "Processing sample number 258\n",
      "Processing sample number 259\n",
      "Processing sample number 260\n",
      "Processing sample number 261\n",
      "Processing sample number 262\n",
      "Processing sample number 263\n",
      "Processing sample number 264\n",
      "Processing sample number 265\n",
      "Processing sample number 266\n",
      "Processing sample number 267\n",
      "Processing sample number 268\n",
      "Processing sample number 269\n",
      "Processing sample number 270\n",
      "Processing sample number 271\n",
      "Processing sample number 272\n",
      "Processing sample number 273\n",
      "Processing sample number 274\n",
      "Processing sample number 275\n",
      "Processing sample number 276\n",
      "Processing sample number 277\n",
      "Processing sample number 278\n",
      "Processing sample number 279\n",
      "Processing sample number 280\n",
      "Processing sample number 281\n",
      "Processing sample number 282\n",
      "Processing sample number 283\n",
      "Processing sample number 284\n",
      "Processing sample number 285\n",
      "Processing sample number 286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sample number 287\n",
      "Processing sample number 288\n",
      "Processing sample number 289\n",
      "Processing sample number 290\n",
      "Processing sample number 291\n",
      "Processing sample number 292\n",
      "Processing sample number 293\n",
      "Processing sample number 294\n",
      "Processing sample number 295\n",
      "Processing sample number 296\n",
      "Processing sample number 297\n",
      "Processing sample number 298\n",
      "Processing sample number 299\n",
      "Processing sample number 300\n",
      "Processing sample number 301\n",
      "Processing sample number 302\n",
      "Processing sample number 303\n",
      "Processing sample number 304\n",
      "Processing sample number 305\n",
      "Processing sample number 306\n",
      "Processing sample number 307\n",
      "Processing sample number 308\n",
      "Processing sample number 309\n",
      "Processing sample number 310\n",
      "Processing sample number 311\n",
      "Processing sample number 312\n",
      "Processing sample number 313\n",
      "Processing sample number 314\n",
      "Processing sample number 315\n",
      "Processing sample number 316\n",
      "Processing sample number 317\n",
      "Processing sample number 318\n",
      "Processing sample number 319\n",
      "Processing sample number 320\n",
      "Processing sample number 321\n",
      "Processing sample number 322\n",
      "Processing sample number 323\n",
      "Processing sample number 324\n",
      "Processing sample number 325\n",
      "Processing sample number 326\n",
      "Processing sample number 327\n",
      "Processing sample number 328\n",
      "Processing sample number 329\n",
      "Processing sample number 330\n",
      "Processing sample number 331\n",
      "Processing sample number 332\n",
      "Processing sample number 333\n",
      "Processing sample number 334\n",
      "Processing sample number 335\n",
      "Processing sample number 336\n",
      "Processing sample number 337\n",
      "Processing sample number 338\n",
      "Processing sample number 339\n",
      "Processing sample number 340\n",
      "Processing sample number 341\n",
      "Processing sample number 342\n",
      "Processing sample number 343\n",
      "Processing sample number 344\n",
      "Processing sample number 345\n",
      "Processing sample number 346\n",
      "Processing sample number 347\n",
      "Processing sample number 348\n",
      "Processing sample number 349\n",
      "Processing sample number 350\n",
      "Processing sample number 351\n",
      "Processing sample number 352\n",
      "Processing sample number 353\n",
      "Processing sample number 354\n",
      "Processing sample number 355\n",
      "Processing sample number 356\n",
      "Processing sample number 357\n",
      "Processing sample number 358\n",
      "Processing sample number 359\n",
      "Processing sample number 360\n",
      "Processing sample number 361\n",
      "Processing sample number 362\n",
      "Processing sample number 363\n",
      "Processing sample number 364\n",
      "Processing sample number 365\n",
      "Processing sample number 366\n",
      "Processing sample number 367\n",
      "Processing sample number 368\n",
      "Processing sample number 369\n",
      "Processing sample number 370\n",
      "Processing sample number 371\n",
      "Processing sample number 372\n",
      "Processing sample number 373\n",
      "Processing sample number 374\n",
      "Processing sample number 375\n",
      "Processing sample number 376\n",
      "Processing sample number 377\n",
      "Processing sample number 378\n",
      "Processing sample number 379\n",
      "Processing sample number 380\n",
      "Processing sample number 381\n"
     ]
    }
   ],
   "source": [
    "current_sample = 0\n",
    "#each column of this matrix is related to a specific attribute\n",
    "negatives_ri_matrix = []\n",
    "for batch in negative_classifier_inputs:\n",
    "    for sample_index in range(len(batch)):\n",
    "        print('Processing sample number {}'.format(current_sample))\n",
    "        current_sample_ris = list(map(lambda att: find_smallest_variation_to_change(model.classifier,\n",
    "                                                                                    classifier_length=classifier_length,\n",
    "                                                                                    attribute_length=attribute_length,\n",
    "                                                                                    input_matrix=batch,\n",
    "                                                                                    vector_index=sample_index,\n",
    "                                                                                    attributes=[attributes.index(att)]\n",
    "                                                                                    ,class_to_reach=1),attributes))\n",
    "        negatives_ri_matrix.append(current_sample_ris)\n",
    "        current_sample+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ri_norms_negative_samples = []\n",
    "for ri_list in negatives_ri_matrix:\n",
    "    ri_norms_negative_samples.append(list(map(lambda x:torch.norm(x).data[0],ri_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives_variation_df = pd.DataFrame(data= ri_norms_negative_samples,columns=attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Beer_Name</th>\n",
       "      <th>Brew_Factory_Name</th>\n",
       "      <th>Style</th>\n",
       "      <th>ABV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.939293</td>\n",
       "      <td>4.829078</td>\n",
       "      <td>7.020437</td>\n",
       "      <td>5.768112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.279866</td>\n",
       "      <td>4.500559</td>\n",
       "      <td>6.484399</td>\n",
       "      <td>5.638288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.098440</td>\n",
       "      <td>1.321770</td>\n",
       "      <td>2.054980</td>\n",
       "      <td>1.806884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.364981</td>\n",
       "      <td>4.146273</td>\n",
       "      <td>6.296639</td>\n",
       "      <td>5.334202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.060275</td>\n",
       "      <td>1.276743</td>\n",
       "      <td>1.965796</td>\n",
       "      <td>1.770278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.681709</td>\n",
       "      <td>4.655130</td>\n",
       "      <td>6.822239</td>\n",
       "      <td>5.641063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4.733634</td>\n",
       "      <td>4.733790</td>\n",
       "      <td>6.686200</td>\n",
       "      <td>5.719144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.352231</td>\n",
       "      <td>4.126837</td>\n",
       "      <td>6.079600</td>\n",
       "      <td>5.402805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.112728</td>\n",
       "      <td>1.325085</td>\n",
       "      <td>1.993112</td>\n",
       "      <td>1.817335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Beer_Name  Brew_Factory_Name     Style       ABV\n",
       "0   0.000000           0.000000  0.000000  0.000000\n",
       "1   4.939293           4.829078  7.020437  5.768112\n",
       "2   4.279866           4.500559  6.484399  5.638288\n",
       "3   1.098440           1.321770  2.054980  1.806884\n",
       "4   2.364981           4.146273  6.296639  5.334202\n",
       "5   1.060275           1.276743  1.965796  1.770278\n",
       "6   4.681709           4.655130  6.822239  5.641063\n",
       "7   4.733634           4.733790  6.686200  5.719144\n",
       "8   2.352231           4.126837  6.079600  5.402805\n",
       "9   1.112728           1.325085  1.993112  1.817335"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negatives_variation_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives_variation_df.to_csv('experiment-results/negatives_ri.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fe8f8621390>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ri_sums = negatives_variation_df.sum(numeric_only=True)\n",
    "ri_sums.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_sample = 0\n",
    "#each column of this matrix is related to a specific attribute\n",
    "positives_ri_matrix = []\n",
    "for batch in positive_classifier_inputs:\n",
    "    for sample_index in range(len(batch)):\n",
    "        print('Processing sample number {}'.format(current_sample))\n",
    "        current_sample_ris = list(map(lambda att: find_smallest_variation_to_change(model.classifier,\n",
    "                                                                                    classifier_length=classifier_length,\n",
    "                                                                                    attribute_length=attribute_length,\n",
    "                                                                                    input_matrix=batch,\n",
    "                                                                                    vector_index=sample_index,\n",
    "                                                                                    attributes=[attributes.index(att)]\n",
    "                                                                                    ,class_to_reach=0),attributes))\n",
    "        positives_ri_matrix.append(current_sample_ris)\n",
    "        current_sample+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ri_norms_positives = []\n",
    "for ri_list in positives_ri_matrix:\n",
    "    ri_norms_positives.append(list(map(lambda x:torch.norm(x).data[0],ri_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives_variation_df = pd.DataFrame(data= ri_norms_positives,columns=attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives_variation_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives_variation_df.to_csv('experiment-results/positives_ri.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ri_sums = positives_variation_df.sum(numeric_only=True)\n",
    "ri_sums.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute closer vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities_functions.distance_measures import nearest_neighbour,nearest_neighbour_onAttribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lista di tuple: vettore pi√π vicino considerando tutti gli elementi e closer solo secondo un attributo\n",
    "negatives_closer_vectors = []\n",
    "i = 0\n",
    "for batch in negative_classifier_inputs:\n",
    "    for sample in batch:\n",
    "        current_sample_closer_vectors = list(map(lambda att: nearest_neighbour_onAttribute\n",
    "                                                 (sample+negatives_ri_matrix[i][attributes.index(att)]\n",
    "                                                                ,positive_classifier_inputs,attributes.index(att),\n",
    "                                                                attribute_length,'cosine'),attributes))\n",
    "        negatives_closer_vectors.append(current_sample_closer_vectors)\n",
    "        i += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per ogni attributo trovo il pi√π vicino\n",
    "positives_closer_vectors = []\n",
    "i = 0\n",
    "for batch in positive_classifier_inputs:\n",
    "    for sample in batch:\n",
    "        current_sample_closer_vectors = list(map(lambda att: nearest_neighbour_onAttribute\n",
    "                                                 (sample+positives_ri_matrix[i][attributes.index(att)]\n",
    "                                                                ,negative_classifier_inputs,attributes.index(att),\n",
    "                                                                attribute_length,'cosine'),attributes))\n",
    "        positives_closer_vectors.append(current_sample_closer_vectors)\n",
    "        i += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives_closer_vectors_df = pd.DataFrame(data= positives_closer_vectors,columns =attributes)\n",
    "positives_closer_vectors_df = positives_closer_vectors_df.applymap(lambda c:neg_ids[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives_closer_vectors_df = pd.DataFrame(data = negatives_closer_vectors, columns = attributes)\n",
    "negatives_closer_vectors_df = negatives_closer_vectors_df.applymap(lambda c:pos_ids[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives_closer_vectors_df['SampleID'] = pos_ids\n",
    "negatives_closer_vectors_df['SampleID'] = neg_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives_closer_vectors_df.to_csv('experiment-results/positives_nn_balanced.csv',index=False)\n",
    "negatives_closer_vectors_df.to_csv('experiment-results/negatives_nn_balanced.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test nearest neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives = pd.read_csv('../../Structured/Beer/negatives.csv')\n",
    "positives = pd.read_csv('../../Structured/Beer/positives.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities_functions.explainer import generateExplanations,analyze_valueDistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives_closer_vectors_df = pd.read_csv(\"experiment-results/positives_nn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>  PREDICT Epoch 3\n",
      "Finished Epoch 3 || Run Time:    0.4 | Load Time:    0.1 || F1:   0.00 | Prec:   0.00 | Rec:   0.00 || Ex/s:   0.00\n",
      "\n",
      "The standard true positives are 59\n",
      "Big Bear Amber Ale | Big Horn Buttface Amber Ale\n",
      "===>  PREDICT Epoch 3\n",
      "Finished Epoch 3 || Run Time:    0.2 | Load Time:    0.1 || F1:   0.00 | Prec:   0.00 | Rec:   0.00 || Ex/s:   0.00\n",
      "\n",
      "2\n",
      "Road Rash Red | Kettle House Bourbon Barrel Road Rash Imperial Red Ale\n",
      "===>  PREDICT Epoch 3\n",
      "Finished Epoch 3 || Run Time:    0.2 | Load Time:    0.1 || F1:   0.00 | Prec:   0.00 | Rec:   0.00 || Ex/s:   0.00\n",
      "\n",
      "10\n",
      "Eruption Imperial Red Ale | De Koninck TSTBRW 01 Imperial Red Ale\n",
      "===>  PREDICT Epoch 3\n",
      "Finished Epoch 3 || Run Time:    0.2 | Load Time:    0.1 || F1:   0.00 | Prec:   0.00 | Rec:   0.00 || Ex/s:   0.00\n",
      "\n",
      "5\n",
      "Flying Monkey Amber Ale | Flying Monkeys Amber Ale\n",
      "===>  PREDICT Epoch 3\n",
      "Finished Epoch 3 || Run Time:    0.2 | Load Time:    0.1 || F1:   0.00 | Prec:   0.00 | Rec:   0.00 || Ex/s:   0.00\n",
      "\n",
      "53\n",
      "Frog Island Amber Ale | Heavy Seas Desert Island Series American Honey Amber Ale\n",
      "===>  PREDICT Epoch 3\n",
      "Finished Epoch 3 || Run Time:    0.2 | Load Time:    0.1 || F1:   0.00 | Prec:   0.00 | Rec:   0.00 || Ex/s:   0.00\n",
      "\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "top5NN = generateExplanations(positives_closer_vectors_df,5,negatives,\"Beer_Name\",model,\n",
    "                    \"../../Structured/Beer/positives.csv\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_positives = pd.read_csv('../../Structured/Beer/test_positives.csv')\n",
    "test_positives['ltable_Beer_Name'] = test_positives['ltable_Beer_Name']+\" Imperial Red Ale\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_positives.to_csv('temp/bias_test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>  PREDICT Epoch 4\n",
      "Finished Epoch 4 || Run Time:    0.1 | Load Time:    0.0 || F1:   0.00 | Prec:   0.00 | Rec:   0.00 || Ex/s:   0.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alttest = dm.data.process_unlabeled(\"temp/bias_test.csv\",model,ignore_columns=['id','label'])\n",
    "pred = model.run_prediction(alttest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>0.688631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>0.549473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>0.684677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>0.681496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     match_score\n",
       "id              \n",
       "577     0.688631\n",
       "579     0.549473\n",
       "574     0.684677\n",
       "646     0.681496"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[pred['match_score']>0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../../Structured/Beer/merged_train.csv\")\n",
    "value = top5NN.iloc[0]['rtable_Beer_Name']\n",
    "analyze_valueDistribution(train,value,\"Beer_Name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = dm.data.process_unlabeled(\"../../Structured/Beer/merged_test.csv\",model,ignore_columns=['id','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.run_prediction(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
